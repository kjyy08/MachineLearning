{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPr8GqcBr4igRPVhMP2wXSh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# problem 2.\n","# cost 함수 그리기\n","# w1 = -5.0 ~ 5.0, w2 = 0, w3 = 0, b = 0으로 고정\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","xy = np.loadtxt('advertising.csv', delimiter=',', dtype=np.float32)\n","x_data = xy[:, 0:-1]\n","y_data = xy[:, [-1]]\n","x_data = np.array(x_data, dtype=np.float32)\n","y_data = np.array(y_data, dtype=np.float32)\n","\n","learning_rate = 1e-5\n","\n","x1_data = xy[:, 0:1]\n","\n","w_list = []\n","cost_list = []\n","\n","# w = -1.0 ~ 1.0으로 0.01단위로 갱신\n","for i in range(-100, 101):\n","  w_curr = i * 0.01\n","  model = w_curr * x1_data  # h(x) = Wx\n","  cost = tf.reduce_mean(tf.square(model - y_data))  # cost function\n","\n","  w_list.append(w_curr)\n","  cost_list.append(cost)\n","  print(\"i: \", i, \"w: \", w_curr, \"cost: \", cost.numpy())\n","\n","plt.plot(w_list, cost_list)\n","plt.xlabel(\"w\")\n","plt.ylabel(\"cost\")\n","plt.show()\n"],"metadata":{"id":"Z6dydgcTg_15"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# problem 3.\n","# 자동 미분 함수 없이 구현\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","xy = np.loadtxt('advertising.csv', delimiter=',', dtype=np.float32)\n","x_data = xy[:, 0:-1]\n","y_data = xy[:, [-1]]\n","x_data = np.array(x_data, dtype=np.float32)\n","y_data = np.array(y_data, dtype=np.float32)\n","\n","W = tf.Variable(tf.random.normal([3, 1]))\n","b = tf.Variable(tf.random.normal([1]))\n","learning_rate = 1e-5\n","\n","def gradientDescent():\n","  model = tf.matmul(x_data, W) + b\n","  gradient = tf.reduce_mean((model - y_data) * x_data)\n","  descent = W - learning_rate * gradient\n","  W.assign(descent)\n","\n","  gradient = tf.reduce_mean((model - y_data))\n","  descent = b - learning_rate * gradient\n","  b.assign(descent)\n","\n","\n","for step in range(10001):\n","  gradientDescent()\n","\n","  if step % 1000 == 0:\n","    model = tf.matmul(x_data, W) + b\n","    cost = tf.reduce_mean(tf.square(model - y_data))\n","\n","    print('step:', step, '\\nW=', W.numpy(), 'b=', b.numpy(), 'cost=', cost.numpy())\n"],"metadata":{"id":"7RG4QPjK9Sz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# problem 4.\n","# 훈련 횟수에 따른 cost값 변화를 나타내기\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","xy = np.loadtxt('advertising.csv', delimiter=',', dtype=np.float32)\n","x_data = xy[:, 0:-1]\n","y_data = xy[:, [-1]]\n","x_data = np.array(x_data, dtype=np.float32)\n","y_data = np.array(y_data, dtype=np.float32)\n","\n","W = tf.Variable(tf.random.normal([3, 1]))\n","b = tf.Variable(tf.random.normal([1]))\n","learning_rate = 1e-5\n","\n","def gradientDescent():\n","  with tf.GradientTape() as tape:\n","    # Model(Hypothesis): H = Wx + b\n","    model = tf.matmul(x_data, W) + b\n","    # Cost function\n","    cost = tf.reduce_mean(tf.square(model - y_data))\n","    # Minimize : Gradient Descent\n","    gradients = tape.gradient(cost,[W,b])\n","    tf.optimizers.SGD(learning_rate).apply_gradients(zip(gradients,[W,b]))\n","\n","step_list = []\n","cost_list = []\n","\n","for step in range(10001):\n","  gradientDescent()\n","\n","  if step % 100 == 0:\n","    model = tf.matmul(x_data, W) + b\n","    cost = tf.reduce_mean(tf.square(model - y_data))\n","\n","    print('step:', step, '\\nW=', W.numpy(), 'b=', b.numpy(), 'cost=', cost.numpy())\n","    step_list.append(step)\n","    cost_list.append(cost.numpy())\n","\n","#plt.scatter(step_list, cost_list)\n","plt.plot(step_list, cost_list)\n","plt.xlabel(\"step\")\n","plt.ylabel(\"cost\")\n","plt.show()"],"metadata":{"id":"yPCP-W6vGtZl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# problem 6.\n","# learning rate에 따른 cost값의 최솟값 변화를 나타내기\n","\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","xy = np.loadtxt('advertising.csv', delimiter=',', dtype=np.float32)\n","x_data = xy[:, 0:-1]\n","y_data = xy[:, [-1]]\n","x_data = np.array(x_data, dtype=np.float32)\n","y_data = np.array(y_data, dtype=np.float32)\n","\n","_W = tf.Variable(tf.random.normal([3, 1]))\n","b = tf.Variable(tf.random.normal([1]))\n","learning_rate = 1e-13\n","point_value = 1e+1\n","\n","W = _W\n","\n","def gradientDescent():\n","  with tf.GradientTape() as tape:\n","    # Model(Hypothesis): H = Wx + b\n","    model = tf.matmul(x_data, W) + b\n","    # Cost function\n","    cost = tf.reduce_mean(tf.square(model - y_data))\n","    # Minimize : Gradient Descent\n","    gradients = tape.gradient(cost,[W,b])\n","    tf.optimizers.SGD(learning_rate).apply_gradients(zip(gradients,[W,b]))\n","\n","learning_rate_list = []\n","cost_list = []\n","\n","# learning rate 1e-13승에서 1e+1씩 값을 증가시키며 10번 반복\n","for i in range(1, 11):\n","  for step in range(1001):\n","    gradientDescent()\n","\n","    if step % 100 == 0:\n","      model = tf.matmul(x_data, W) + b\n","      cost = tf.reduce_mean(tf.square(model - y_data))\n","\n","      #print('step:', step, '\\nW=', W.numpy(), 'b=', b.numpy(), 'cost=', cost.numpy())\n","\n","  cost_list.append(cost.numpy())\n","  learning_rate_list.append(learning_rate)\n","  learning_rate = learning_rate * point_value\n","  W = _W\n","\n","print(learning_rate_list)\n","print(\"cost list: \", cost_list)\n","plt.scatter(learning_rate_list, cost_list)\n","plt.plot(learning_rate_list, cost_list)\n","plt.xlabel(\"learing_rate\")\n","plt.ylabel(\"cost\")\n","plt.show()"],"metadata":{"id":"gN1311Yw56aQ"},"execution_count":null,"outputs":[]}]}