{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMn/s+ftt4qG8+yjPH9PFMF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"o_NLN_GCIi3k"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import time\n","from tensorflow.keras.datasets import mnist\n","\n","batch_size = 128\n","nH1 = 256 # layer1\n","nH2 = 256 # layer2\n","nH3 = 256 # layer3\n","\n","(x_train, y_train),(x_test,y_test) = tf.keras.datasets.mnist.load_data()\n","x_train, x_test = x_train.astype('float32'), x_test.astype('float32')\n","x_train, x_test = x_train.reshape([-1, 784]), x_test.reshape([-1,784])\n","x_train, x_test = x_train/255, x_test/255\n","y_train, y_test = tf.one_hot(y_train, depth=10), tf.one_hot(y_test,depth=10)\n","train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_data = train_data.shuffle(60000).batch(batch_size)\n","\n","class ANN(object):\n","  def __init__(self):\n","    self.W1 = tf.Variable(tf.random.normal(shape=[784,nH1]))\n","    self.W2 = tf.Variable(tf.random.normal(shape=[nH1,nH2]))\n","    self.W3 = tf.Variable(tf.random.normal(shape=[nH2,nH3]))\n","    self.Wout = tf.Variable(tf.random.normal(shape=[nH3,10]))\n","\n","    self.b1 = tf.Variable(tf.random.normal(shape=[nH1]))\n","    self.b2 = tf.Variable(tf.random.normal(shape=[nH2]))\n","    self.b3 = tf.Variable(tf.random.normal(shape=[nH3]))\n","    self.bout = tf.Variable(tf.random.normal(shape=[10]))\n","\n","  def __call__(self, x):\n","    # H(x) = Wx + b, 활성화 함수로 ReLu 사용\n","    H1_out = tf.nn.relu(tf.matmul(x, self.W1) + self.b1)\n","    H2_out = tf.nn.relu(tf.matmul(H1_out, self.W2) + self.b2)\n","    H3_out = tf.nn.relu(tf.matmul(H2_out, self.W3) + self.b3)\n","    out = tf.matmul(H3_out, self.Wout) + self.bout\n","\n","    return out\n","\n","ANN_model = ANN()\n","\n","optimizer = tf.optimizers.Adam(0.01)\n","\n","@tf.function\n","def accuracy(x, y):\n","  correct = tf.equal(tf.argmax(x,1), tf.argmax(y,1))\n","  accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n","  return accuracy\n","\n","@tf.function\n","def train_step(model, x, y):\n","  with tf.GradientTape() as tape:\n","    y_pred = model(x)\n","    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y))\n","    gradients = tape.gradient(loss, vars(model).values())\n","    optimizer.apply_gradients(zip(gradients, vars(model).values()))\n","\n","for epoch in range(101):\n","  avg_loss = 0\n","  tot_batch = int(x_train.shape[0] / batch_size)\n","  for batch_x, batch_y in train_data:\n","    train_step(ANN_model, batch_x, batch_y)\n","    current_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=ANN_model(batch_x), labels=batch_y))\n","    #__, current_loss = train_step(ANN_model, batch_x, batch_y), tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=ANN_model(batch_x), labels=batch_y))\n","    avg_loss = avg_loss + current_loss/tot_batch\n","\n","  if epoch % 5 == 0:\n","    print(\"Step: %d, Loss = %f\" %(epoch, avg_loss))\n","    print(\"Accuracy of test: %f\" %accuracy(ANN_model(x_test), y_test), end=\"\\n----------------------------------------------------------\\n\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"5T3WkDYkJSMq"},"execution_count":null,"outputs":[]}]}