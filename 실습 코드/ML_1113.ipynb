{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPt92FSwPjvDjKK4JMAnfBa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qiIHxSBldj6p","outputId":"090df20d-cd93-4e7b-8cb3-6a90c3fe86a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 4, Iteration: 240, Loss = 0.034766, Accuracy of test: 0.965300\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras import Model, layers\n","\n","batch_size = 128\n","conv1_filters = 32\n","conv2_filters = 64\n","fc1_units = 1024\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data() # load MNIST data\n","x_train, x_test = np.array(x_train,np.float32), np.array(x_test,np.float32)\n","x_train, x_test = x_train/255, x_test/255\n","train_data = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n","train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)\n","\n","class CNN(Model):\n","  def __init__(self):\n","    super(CNN, self).__init__()\n","    self.conv1 = layers.Conv2D(32, kernel_size=3, activation=tf.nn.relu)\n","    self.maxpool1 = layers.MaxPool2D(2, strides=2)\n","    self.conv2 = layers.Conv2D(64, kernel_size=3, activation=tf.nn.relu)\n","    self.maxpool2 = layers.MaxPool2D(2, strides=2)\n","    self.flatten = layers.Flatten()\n","    self.fc1 = layers.Dense(1024)\n","    self.out = layers.Dense(10)\n","  def call(self,x):\n","      x = tf.reshape(x,[-1, 28, 28, 1])\n","      x = self.conv1(x)\n","      x = self.maxpool1(x)\n","      x = self.conv2(x)\n","      x = self.maxpool2(x)\n","      x = self.flatten(x)\n","      x = self.fc1(x)\n","      x = self.out(x)\n","\n","      return x\n","\n","CNN_model = CNN()\n","optimizer = tf.optimizers.Adam(0.01)\n","\n","@tf.function\n","def cross_entropy_loss(x, y):\n","  y = tf.cast(y, tf.int64)\n","  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n","  return tf.reduce_mean(loss)\n","\n","@tf.function\n","def accuracy(x, y):\n","  correct = tf.equal(tf.argmax(x, 1), tf.cast(y, tf.int64))\n","  return tf.reduce_mean(tf.cast(correct, tf.float32), axis=-1)\n","\n","@tf.function\n","def train_step(x,y):\n","  with tf.GradientTape() as g:\n","    y_pred = CNN_model(x)\n","    loss = cross_entropy_loss(y_pred, y)\n","    trainable_variables = CNN_model.trainable_variables\n","    gradients = g.gradient(loss, trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, trainable_variables))\n","\n","#avg_loss = 0\n","#tot_batch = int(x_train.shape[0] / batch_size)\n","\n","for epoch in range(10):\n","  avg_loss = 0\n","  tot_batch = int(x_train.shape[0] / batch_size)\n","  for iter, (batch_x, batch_y) in enumerate(train_data.take(300), 1):\n","    train_step(batch_x, batch_y)\n","    current_loss = cross_entropy_loss(CNN_model(batch_x), batch_y)\n","    avg_loss = avg_loss + current_loss / tot_batch\n","\n","    if iter % 10 == 0:\n","      print(\"Epoch: %d, Iteration: %d, Loss = %f, Accuracy of test: %f\"% (epoch, iter, avg_loss, accuracy(CNN_model(x_test), y_test)))"]},{"cell_type":"code","source":[],"metadata":{"id":"TWMfzeu-MBbm"},"execution_count":null,"outputs":[]}]}