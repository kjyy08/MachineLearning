{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGOXnzX4YYpSlc9v3iLbYb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"shxjzGztJq4t","executionInfo":{"status":"ok","timestamp":1698647348142,"user_tz":-540,"elapsed":220885,"user":{"displayName":"김주엽","userId":"12669261044801974628"}},"outputId":"e9136c8f-c4e6-47eb-ed9e-375d9b82ff47"},"outputs":[{"output_type":"stream","name":"stdout","text":["step:  0 \n","hyp:  [[0.32304403]\n"," [0.320327  ]\n"," [0.3280456 ]\n"," [0.32518277]] \n","prediction:  [[0.]\n"," [0.]\n"," [0.]\n"," [0.]] \n","accuracy:  0.5\n","---------------------------------\n","step:  1000 \n","hyp:  [[0.49477896]\n"," [0.49605146]\n"," [0.50757784]\n"," [0.50417095]] \n","prediction:  [[0.]\n"," [0.]\n"," [1.]\n"," [1.]] \n","accuracy:  0.5\n","---------------------------------\n","step:  2000 \n","hyp:  [[0.481615  ]\n"," [0.49755165]\n"," [0.5136158 ]\n"," [0.5112773 ]] \n","prediction:  [[0.]\n"," [0.]\n"," [1.]\n"," [1.]] \n","accuracy:  0.5\n","---------------------------------\n","step:  3000 \n","hyp:  [[0.36333686]\n"," [0.5284249 ]\n"," [0.54962224]\n"," [0.5645066 ]] \n","prediction:  [[0.]\n"," [1.]\n"," [1.]\n"," [1.]] \n","accuracy:  0.75\n","---------------------------------\n","step:  4000 \n","hyp:  [[0.09042049]\n"," [0.6334668 ]\n"," [0.6820802 ]\n"," [0.57970285]] \n","prediction:  [[0.]\n"," [1.]\n"," [1.]\n"," [1.]] \n","accuracy:  0.75\n","---------------------------------\n","step:  5000 \n","hyp:  [[0.02046607]\n"," [0.9628065 ]\n"," [0.9527821 ]\n"," [0.05269014]] \n","prediction:  [[0.]\n"," [1.]\n"," [1.]\n"," [0.]] \n","accuracy:  1.0\n","---------------------------------\n","step:  6000 \n","hyp:  [[0.0063931 ]\n"," [0.9889212 ]\n"," [0.9858816 ]\n"," [0.01460753]] \n","prediction:  [[0.]\n"," [1.]\n"," [1.]\n"," [0.]] \n","accuracy:  1.0\n","---------------------------------\n","step:  7000 \n","hyp:  [[0.00355337]\n"," [0.99387395]\n"," [0.9922368 ]\n"," [0.00778579]] \n","prediction:  [[0.]\n"," [1.]\n"," [1.]\n"," [0.]] \n","accuracy:  1.0\n","---------------------------------\n","step:  8000 \n","hyp:  [[0.00241998]\n"," [0.9958277 ]\n"," [0.994738  ]\n"," [0.00517522]] \n","prediction:  [[0.]\n"," [1.]\n"," [1.]\n"," [0.]] \n","accuracy:  1.0\n","---------------------------------\n","step:  9000 \n","hyp:  [[0.00182254]\n"," [0.9968534 ]\n"," [0.99604726]\n"," [0.00383309]] \n","prediction:  [[0.]\n"," [1.]\n"," [1.]\n"," [0.]] \n","accuracy:  1.0\n","---------------------------------\n","step:  10000 \n","hyp:  [[0.0014568 ]\n"," [0.99748045]\n"," [0.9968453 ]\n"," [0.00302575]] \n","prediction:  [[0.]\n"," [1.]\n"," [1.]\n"," [0.]] \n","accuracy:  1.0\n","---------------------------------\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n","y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n","\n","W1 = tf.Variable(tf.random.normal([2, 4]))\n","b1 = tf.Variable(tf.random.normal([4]))\n","\n","W2 = tf.Variable(tf.random.normal([4, 4]))\n","b2 = tf.Variable(tf.random.normal([4]))\n","\n","W3 = tf.Variable(tf.random.normal([4, 4]))\n","b3 = tf.Variable(tf.random.normal([4]))\n","\n","W4 = tf.Variable(tf.random.normal([4, 1]))\n","b4 = tf.Variable(tf.random.normal([1]))\n","\n","\n","learning_rate = 0.1\n","\n","def xor_nn():\n","  with tf.GradientTape() as tape:\n","    layer1 = tf.sigmoid(tf.matmul(x_data, W1) + b1)\n","    layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n","    layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n","    hyp = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n","    cost = -tf.reduce_mean(y_data * tf.math.log(hyp) + (1 - y_data) * tf.math.log(1 - hyp))\n","    gradients = tape.gradient(cost, [W4, b4, W3, b3, W2, b2, W1, b1])\n","    tf.optimizers.SGD(learning_rate).apply_gradients(zip(gradients, [W4, b4, W3, b3, W2, b2, W1, b1]))\n","\n","for step in range(10001):\n","  xor_nn()\n","  layer1 = tf.sigmoid(tf.matmul(x_data, W1) + b1)\n","  layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n","  layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n","  hyp = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n","  cost = -tf.reduce_mean(y_data * tf.math.log(hyp) + (1 - y_data) * tf.math.log(1 - hyp))\n","  prediction = tf.cast(hyp > 0.5, dtype=tf.float32) # tf.cast(): 0.5보다 작으면 0, 크면 1을 반환\n","  accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_data), dtype= np.float32))\n","\n","  if step % 1000 == 0:\n","    print(\"step: \", step, \"\\nhyp: \", hyp.numpy(), \"\\nprediction: \", prediction.numpy(), \"\\naccuracy: \", accuracy.numpy(), end=\"\\n---------------------------------\\n\")\n","\n","\n"]},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","x_data = np.array([[0.1, 0.2]], dtype=np.float32)\n","y_data = np.array([[0.4, 0.6]], dtype=np.float32)\n","\n","w1 = 0.3\n","w2 = 0.25\n","w3 = 0.4\n","w4 = 0.35\n","w5 = 0.45\n","w6 = 0.4\n","w7 = 0.7\n","w8 = 0.6\n","\n","W1 = tf.Variable(tf.constant([[w1, w2], [w3, w4]]), dtype=np.float32)\n","W2 = tf.Variable(tf.constant([[w5, w6], [w7, w8]]), dtype=np.float32)\n","\n","\n","learning_rate = 0.1\n","\n","def nn():\n","  with tf.GradientTape() as tape:\n","    t_layer1 = tf.matmul(x_data, W1)\n","    z_layer1 = tf.sigmoid(t_layer1)\n","    t_layer2 = tf.matmul(z_layer1, W2)\n","    z_layer2 = tf.sigmoid(t_layer2)\n","\n","    error = tf.reduce_sum(tf.square(y_data - z_layer2))\n","\n","    gradients = tape.gradient(error, [W2, W1])\n","    tf.optimizers.SGD(learning_rate).apply_gradients(zip(gradients, [W2, W1]))\n","\n","for step in range(3):\n","\n","  nn()\n","  t_layer1 = tf.matmul(x_data, W1)\n","  z_layer1 = tf.sigmoid(t_layer1)\n","  t_layer2 = tf.matmul(z_layer1, W2)\n","  z_layer2 = tf.sigmoid(t_layer2)\n","\n","  error = tf.reduce_sum(tf.square(y_data - z_layer2))\n","\n","  print(\"W2: \", W2.numpy(), \"\\nW1: \", W1.numpy(), \"\\nerror: \", error.numpy(), end=\"\\n-----------------------------------------------------------\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8_TNAbd6biR2","executionInfo":{"status":"ok","timestamp":1698650919150,"user_tz":-540,"elapsed":359,"user":{"displayName":"김주엽","userId":"12669261044801974628"}},"outputId":"279de7d6-2abb-46e4-e3a2-653bdd5eda90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["W2:  [[0.44405594 0.3993011 ]\n"," [0.69409806 0.5993061 ]] \n","W1:  [[0.29986042 0.24978341]\n"," [0.3997208  0.34956682]] \n","error:  0.060886007\n","-----------------------------------------------------------\n","W2:  [[0.43813577 0.39860645]\n"," [0.68821996 0.59861636]] \n","W1:  [[0.2997231  0.2495694 ]\n"," [0.39944613 0.3491388 ]] \n","error:  0.060178667\n","-----------------------------------------------------------\n","W2:  [[0.43223965 0.397916  ]\n"," [0.6823659  0.59793085]] \n","W1:  [[0.299588   0.24935795]\n"," [0.39917597 0.3487159 ]] \n","error:  0.059477165\n","-----------------------------------------------------------\n"]}]}]}