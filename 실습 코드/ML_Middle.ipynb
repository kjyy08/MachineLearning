{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNwRlmGMApYYs92wP44LQjT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"FgczQL90f-6O"},"outputs":[],"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","x = np.array([1, 2, 3, 4], dtype=np.float32)\n","y = np.array([2, 1, 4, 3], dtype=np.float32)\n","\n","W = tf.Variable([-1.0063403])\n","b = tf.Variable([-1.5659915])\n","\n","learning_rate = 1e-4\n","\n","def gradientDescent():\n","  model = W * x + b\n","\n","  descent = W - learning_rate * tf.reduce_mean((model - y) * x)\n","  W.assign(descent)\n","\n","  descent = b - learning_rate * tf.reduce_mean((model - y))\n","  b.assign(descent)\n","\n","for i in range(1501):\n","  gradientDescent()\n","  model = W * x + b\n","  cost = tf.reduce_mean(tf.square(model - y))\n","\n","  if (i % 50) == 0:\n","    print(i, W.numpy(), b.numpy(), cost.numpy())\n","\n"]},{"cell_type":"code","source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","xy = np.loadtxt(\"advertising.csv\", delimiter=',', dtype=np.float32)\n","\n","x_data = np.array(xy[:, 0:-1])\n","y_data = np.array(xy[:, [-1]])\n","\n","W = tf.Variable(tf.random.normal([3, 1]))\n","b = tf.Variable(tf.random.normal([1]))\n","learning_rate = 1e-5\n","\n","def gradientDescent():\n","  with tf.GradientTape() as tape:\n","    model = tf.matmul(x_data, W) + b\n","    cost = tf.reduce_mean(tf.square(model - y_data))\n","    gradients = tape.gradient(cost, [W, b])\n","\n","    tf.optimizers.SGD(learning_rate).apply_gradients(zip(gradients, [W, b]))\n","\n","def _gradientDescent():\n","  model = tf.matmul(x_data, W) + b\n","\n","  descent = W - learning_rate * tf.reduce_mean((model - y_data) * x_data)\n","  W.assign(descent)\n","\n","  descent = b - learning_rate * tf.reduce_mean(model - y_data)\n","  b.assign(descent)\n","\n","for i in range(10001):\n","  _gradientDescent()\n","  model = tf.matmul(x_data, W) + b\n","  cost = tf.reduce_mean(tf.square(model - y_data))\n","\n","  if (i % 50) == 0:\n","    print(i, W.numpy(), b.numpy(), cost.numpy())\n","\n","\n"],"metadata":{"id":"B0N_hMGyrk8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","xy = np.loadtxt(\"data-03-diabetes.csv\", delimiter=',', dtype=np.float32)\n","\n","x_data = np.array(xy[:, 0:-1])\n","y_data = np.array(xy[:, [-1]])\n","\n","W = tf.Variable(tf.random.normal([8, 1]))\n","b = tf.Variable(tf.random.normal([1]))\n","\n","learning_rate = 1e-2\n","\n","def logisticRegression():\n","  with tf.GradientTape() as tape:\n","    hyp = tf.sigmoid(tf.matmul(x_data, W) + b)\n","    cost = -tf.reduce_mean(y_data * tf.math.log(hyp) + (1 - y_data) * tf.math.log(1 - hyp))\n","    gradients = tape.gradient(cost, [W, b])\n","    tf.optimizers.SGD(learning_rate).apply_gradients(zip(gradients, [W, b]))\n","\n","for i in range(10001):\n","  logisticRegression()\n","  hyp = tf.sigmoid(tf.matmul(x_data, W) + b)\n","  cost = -tf.reduce_mean(y_data * tf.math.log(hyp) + (1 - y_data) * tf.math.log(1 - hyp))\n","  prediction = tf.cast(hyp > 0.5, dtype=np.float32)\n","  accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_data), tf.float32))\n","\n","  if i % 1000 == 0:\n","    print(i, accuracy.numpy(), cost.numpy())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zUoSD3gzHOQ0","executionInfo":{"status":"ok","timestamp":1698027337375,"user_tz":-540,"elapsed":70988,"user":{"displayName":"김주엽","userId":"12669261044801974628"}},"outputId":"65578ece-03b0-48bb-d70e-1a360e035c71"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.65217394 1.147624\n","1000 0.65217394 0.6029267\n","2000 0.7061924 0.5637867\n","3000 0.7259552 0.5384973\n","4000 0.7509881 0.52135944\n","5000 0.75625825 0.509387\n","6000 0.76679844 0.5008024\n","7000 0.7641634 0.49450922\n","8000 0.76811594 0.48980752\n","9000 0.76811594 0.48623717\n","10000 0.770751 0.4834872\n"]}]},{"cell_type":"code","source":["xy = np.loadtxt(\"data-04-zoo.csv\", delimiter=',', dtype=np.float32)\n","\n","x_data = np.array(xy[:, 0:-1])\n","y_data = np.array(xy[:, [-1]])\n","\n","_y_data_one_hot = tf.one_hot(y_data, 7)\n","y_data_one_hot = tf.reshape(_y_data_one_hot, [-1, 7])\n","\n","W = tf.Variable(tf.random.normal([16, 7]))\n","b = tf.Variable(tf.random.normal([7]))\n","\n","learning_rate = 1e-1\n","\n","def softmax_classfier():\n","  with tf.GradientTape() as tape:\n","    logits = tf.matmul(x_data, W) + b\n","    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_data_one_hot))\n","    gradients = tape.gradient(cost, [W, b])\n","    tf.optimizers.SGD(learning_rate).apply_gradients(zip(gradients, [W, b]))\n","\n","for i in range(10001):\n","  softmax_classfier()\n","  logits = tf.matmul(x_data, W) + b\n","  model = tf.argmax(tf.nn.softmax(logits), 1)\n","  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_data_one_hot))\n","  accuracy = tf.reduce_mean(tf.cast(tf.equal(model, tf.argmax(y_data_one_hot, 1)), dtype=tf.float32))\n","\n","  if i % 1000 == 0:\n","    print(i, model.numpy(), \"\\n\", cost.numpy(), accuracy.numpy())\n","\n"],"metadata":{"id":"o-o5wCFPYScV"},"execution_count":null,"outputs":[]}]}